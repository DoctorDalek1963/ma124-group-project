{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACE8bKQ9Cv_5"
   },
   "source": [
    "#### MA124 Maths by Computer\n",
    "# Project: Constructing and applying Machine Learning models\n",
    "\n",
    "#### Background\n",
    "Machine Learning is the use and development of computer systems that are able to learn and adapt without following explicit instructions, by using algorithms and statistical models to analyse and draw inferences from patterns in data. In machine learning we are interested in creating functions or models which fit real data. Such functions can be found by making them dependent on a number of parameters which are updated over and over again to improve the fit to the data. You will explore in this project.\n",
    "\n",
    "#### Suggested reading\n",
    "\n",
    "There are many great articles, books, videos about machine learning. This list just picks out a few:\n",
    "\n",
    "[Linear Regression Model](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fbuiltin.com%2Fdata-science%2Fregression-machine-learning&data=05%7C02%7Cr.m.lissaman%40warwick.ac.uk%7Cd1a1c7aea6f24f64013008dc11660633%7C09bacfbd47ef446592653546f2eaf6bc%7C0%7C0%7C638404377086620510%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=qRO9I25r9ACsSVXLa0vJXZNj%2BTK4Ha2iW7HzGoEKYlc%3D&reserved=0). Short, simple article covering a task similar to A2 below.\n",
    "\n",
    "[Logistic Regression Model](http://cs231n.github.io/neural-networks-case-study/) An article which covers models similar to those in A3 and A4 below.\n",
    "\n",
    "[What is a neural network](https://www.youtube.com/watch?v=aircAruvnKk). Great YouTube series of introductory videos about training neural networks by the excellent 3Blue1Brown, well worth watching.\n",
    "\n",
    "[Blog: An Introduction to Neural Networks](https://victorzhou.com/blog/intro-to-neural-networks/). Good discussion of the basic ideas used in this project. \n",
    "\n",
    "[Hands on Machine Learning with Python](https://0-link-springer-com.pugwash.lib.warwick.ac.uk/book/10.1007/978-1-4842-7921-2) This might accompany a full course on machine learning. It has much more detail that you will need. As the title says, it's all done using Python. It's well-written and popular.\n",
    "\n",
    "#### Struture of project\n",
    "\n",
    "There are 6 tasks in the document below, tasks A1-A4 in Section A and then tasks B1 and B2 in section B. Your group should do all the  tasks in section A and at least one of the two tasks in section B. A1-A4 are worth approximately 60% of the credit for this submission and section B is worth approximately the other 40%.\n",
    "\n",
    "#### Notes on submission\n",
    "Read through the document **MA124 Maths by Computer Tutor Group Projects Information for Students.pdf** on the MA124 Moodle page.\n",
    "\n",
    "Before submitting see the notes at the end of this document.\n",
    "\n",
    "#### Allowed libraries for this project:\n",
    "numpy matplotlib copy pandas scikitlearn. **Note you may not use scikitlearn in any of tasks A2, A3 or A4.**\n",
    "\n",
    "This means that you may include these lines of code at the start of of your code cells (and please use the aliases given below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import copy\n",
    "import sklearn # Not to be used in any of tasks A2, A3 or A4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ59KgiHCv_8"
   },
   "source": [
    "## Section A - (worth approximately 60% of the marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvcq6JPLCv_8"
   },
   "source": [
    "### Task A1 - Linear Regression machine learning task using SciKitLearn (worth approximately 20% of the marks)\n",
    "\n",
    "In a recent research article published in the journal Computer Communications, authors Sathishkumar V E, Jangwoo Park, and Yongyun Cho sought to predict the \"bike count required at each hour for the stable supply of rental bikes\"[1]. They employed several regression models, including linear regression. The dataset used in the original study is available [here](https://archive.ics.uci.edu/ml/datasets/Seoul+Bike+Sharing+Demand).\n",
    "\n",
    "**Task:** Apply machine learning to a modified version of the original dataset and report the results.\n",
    "\n",
    "\n",
    "[1] Sathishkumar V E, Jangwoo Park, and Yongyun Cho. 'Using data mining techniques for bike sharing demand prediction in metropolitan city.' Computer Communications, Vol.153, pp.353-366, March, 2020 [web link](https://doi.org/10.1016/j.comcom.2020.02.007).\n",
    "\n",
    "---\n",
    "\n",
    "The original research article and a modified dataset are posted on the MA124 Moodle page in the resources for this project. You will need to refer to the article for some of the tasks below. You will need to download SeoulBikeData_mod.csv and put it into the folder with this file.\n",
    "\n",
    "SeoulBikeData_mod.csv has been modified from the original dataset to remove the categorical variables, and to convert dates to months. Months have been coded by number, e.g. 1 = January, etc. Only half the months are included in the modified dataset.\n",
    "\n",
    "---\n",
    "\n",
    "While the number of tasks is large, this is in part because the instructions are rather specific. Many of them are very similar to things you saw in term 1, in the week 8 and week 10 notebooks and lectures.\n",
    "\n",
    "**Specifically, your code should**\n",
    "\n",
    "1. Import needed libraries. (You will need pandas, as well as things from sklearn, and of course numpy and matplotlib.)\n",
    "\n",
    "2. Using pandas, read SeoulBikeData_mod.csv into a Dataframe.\n",
    "\n",
    "3. `describe` the Dataframe.\n",
    "\n",
    "4. Plot a histogram of `Rented Bike Count`. Do not plot this as a density, but as a count. See Fig. 3 of the article. The vertical axis in the article is labelled \"frequency\", but is the same as the count.\n",
    "\n",
    "    Produce a box plot similar to that in Fig. 3 of the article.\n",
    "    \n",
    "    Try to generate both the histogram and box plot to look approximately as they do in the article.\n",
    "\n",
    "5. From the full Dataframe, create a new Dataframe `X` containing all the columns except `Rented Bike Count` and a Series `y` containing only the `Rented Bike Count` column. These are your design matrix and target respectively.\n",
    "\n",
    "6. Perform a test-train split to create `X_train`, `X_test`, `y_train` and `y_test`. You **must** use the same percentage of data for testing and training as was used in the article and you **must** state what they are. You can find these in the article.\n",
    "\n",
    "7. Create and train a linear regression model.\n",
    "\n",
    "8. Use the trained model to obtain `y_pred`, the prediction on the test data `X_test`. Form the residual `resid = y_test - y_pred`.\n",
    "\n",
    "9. Compute and report: Rsquared (R2), the Root Mean Squared Error (RMSE), the Mean Absolute Error (MAE), and the Coefficient of Variation (CV). Compare these results to those on the top, right of Table 4 of the article. (Note, the modified dataset we are studying is different from that used in the article. Hence the results will not be identical. However, the procedure is very close to that used in the article.)\n",
    "\n",
    "10. Produce and comment on the following plots.\n",
    "\n",
    "- Histograms of `y_test` and of `y_pred` (on the same plot). These should be reported as counts rather than densities.\n",
    "\n",
    "- A scatter plot of `resid` as a function of `y_test` corresponding to Fig. 9 of the article. (Recall what `y_test` represents and label the plot appropriately.) Unlike Fig. 9 of the paper, you should use a colormap to plot the different `Hours` in different colours.\n",
    "\n",
    "- A scatter plot of `resid` as a function of `X_test['Month']`. Use a colormap to indicate the absolute value of `resid`.\n",
    "\n",
    "- A scatter plot of `resid` as a function of `X_test['Rainfall(mm)']`. Use a colormap to indicate the absolute value of `resid`.\n",
    "\n",
    "*Insert code and markdown cells below in which to answer this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIzMRAGWCv_9"
   },
   "source": [
    "### Task A2 Linear Regression, on a very small dataset, from scratch with only numpy (worth approximately 10% of the marks)\n",
    "#### (illustrating idea of rates of change with respect to the model parameters and gradient descent)\n",
    "\n",
    "Consider a really small dataset consisting of only three points in the plane: $(1,1),(2,4),(3,5)$.\n",
    "\n",
    "For each point (training example) consider the $x$-coordinate as a single input feature, and the $y$-coordinate as the output or label of the example. The objective is to find the best line to fit these data. Here 'best' will mean the line which minimises the mean of the sum of the squares of the residuals with respect to the three points.\n",
    "\n",
    "In this task and in the following tasks A3 and A4 we will use a slightly different convention to store our data with respect to the one used in A1 or the lectures. $X_{\\text{train}}$ and $y_{\\text{train}}$ will now be the transpose of the corresponding matrices in A1. In A3 and A4 this will be a more convenient form for applying functions given by matrices to the data with the matrix on the left of the data matrix.\n",
    "\n",
    "The feature matrix $X_{\\text{train}}$ contains the feature vectors for all the training examples. More precisely $X_{\\text{train}}= \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} $ has one column for each training example and each column is the feature vector for that training example (in this case we have only one feature which is the $x$-coordinate).\n",
    "\n",
    "The labels matrix $y_{\\text{train}}$ contains the labels for all the training examples. More precisely $y_{\\text{train}} = \\begin{pmatrix} 1 & 4 & 5\\end{pmatrix}$ has one column for each training example and each column contains the label for that training example (the $y$-coordinate).\n",
    "\n",
    "The model here is a simple linear model\n",
    "\n",
    "$$\n",
    "\\hat y = mx +c\n",
    "$$\n",
    "\n",
    "with trainable parameters $m$ and $c$. The goal is to find the optimal values of $m$ and $c$ to fit these training data. In the Machine Learning context this means defining a loss function between prediction and true label:\n",
    "\n",
    "$$\n",
    "L(\\hat y, y)=(\\hat y - y)^2\n",
    "$$\n",
    "\n",
    "and a cost function which is simply the average on all training examples of these losses.\n",
    "\n",
    "**Specifically, you should**\n",
    "\n",
    "1. Show, in a markdown cell, how the cost $J$ depends only on the trainable parameters $m$ and $c$, and can be computed to be:\n",
    "$$ J=\\displaystyle\\frac{14m^2+12cm-48m+3c^2-20c+42}{3}$$\n",
    "\n",
    "2. Find, in a markdown cell, expressions for the partial derivatives $\\displaystyle\\frac{\\partial J}{\\partial m}$ and $\\displaystyle\\frac{\\partial J}{\\partial c}$.\n",
    "3. Write a function `model1(alpha, num_iterations)` which takes as inputs a  learning rate `alpha` and a number of iterations `num_iterations`, and returns optimized values of $m$ and $c$ through Gradient Descent. More precisely the function should initialize $m$ and $c$ randomly between $-2$ and $2$ and then perform `num_iterations` steps of gradient descent with learning rate `alpha`. This means that the value of $m$ is updated to $m - \\displaystyle\\frac{\\partial J}{\\partial m}\\alpha$ and the value of $c$ is updated to $c - \\displaystyle\\frac{\\partial J}{\\partial c}\\alpha$ in each iteration.\n",
    "Your function should print the cost $J$ periodically throughout the iteration process. You may wish to refer to the Machine learning project lecture for help with this. You may wish to build helper functions for the various tasks you need this function to do and refer to them.\n",
    "4. Plot the 3 datapoints along with the optimal line $mx+c$ your model has found.\n",
    "5. Explore the effect of changing the learning rate and the number of iterations.\n",
    "\n",
    "*Insert code and markdown cells here in which to answer this task*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swafy0AtCv_9"
   },
   "source": [
    "### Task A3 Machine Learning for binary classification using Logistic Regression (worth approximately 10% of the marks)\n",
    "\n",
    "Here you will start by running some code provided to you which will create a dataset. You will use this to train a Logistic Regression classification model. \n",
    "\n",
    "The dataset consists of a set of points $(x_0, x_1)$ in the plane each of which is either blue or red. Your model will take as input the coordinates of a point in the plane and return 0 if the point is likely to be red, or 1 if the point is likely to be blue. In practice your model will output a float (real number) between 0 and 1 which is the probability for the point to be blue, and you will then use 0.5 as a threshold to predict the point as being blue or red.\n",
    "\n",
    "**You are guided through this task below. You will see code cells that you need to run and code cells that you need to edit/complete. You can also add new code/markdown cells as required. You may wish to add markdown cells to comment on your code or to comment on any outputs you see.**\n",
    "\n",
    "#### Create and visualise the dataset\n",
    "\n",
    "**Run the cell below to create a planar dataset.** A visualisation of this is provided, you will see that it looks like a flower with some red points and some blue points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "qVndz7rIQtvj",
    "outputId": "8b8da36c-5c1c-4258-ef44-5487a51445c4"
   },
   "outputs": [],
   "source": [
    "# Create the flower dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_flower_dataset():\n",
    "    np.random.seed(1)\n",
    "\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "\n",
    "    X = X.T\n",
    "    y = y.T\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Load the dataset:\n",
    "X, y = load_flower_dataset()\n",
    "\n",
    "# Visualize the dataset:\n",
    "plt.scatter(X[0, :], X[1, :], c=y, s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YaeLo6OwO9Q"
   },
   "source": [
    "You will see that you have some red points (which have label $y=0$) and some blue points (which have label $y=1$).\n",
    "\n",
    "More precisely you have created two vectors `X` and `y` (shorthands for $X_{\\text{train}}$ and $y_{\\text{train}}$) containing 400 training examples:\n",
    "- The numpy-array `X` of dimensions (2, 400), every column of `X` is the feature vector for a single training example and contains the coordinates of the point: $\\begin{pmatrix} x_0 \\\\ x_1\\end{pmatrix}$.\n",
    "- The numpy-array `y` of dimensions (1,400), every column of `y` contains the label for a single training example (0 for red, 1 for blue). \n",
    "- The columns of `X` correspond to the columns of `y`, i.e. the point in the first column of `X` corresponds to the first entry in `y` and so on.\n",
    "\n",
    "Your goal is to build a model to fit this data. In other words, you want the model to define regions of the plane as either red or blue. Red regions will correspond to points $(x_0,x_1)$ where the model returns a value less than 0.5 and with any other parts of the plane being blue regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t2aGfJwlnL5"
   },
   "source": [
    "Let's start by building a logistic regression model and then measuring its performance on this problem. You will build it in numpy from scratch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BPaJL8DKfoM"
   },
   "source": [
    "These are the steps to build your model.\n",
    "\n",
    "1. Define the structure:\n",
    "$$\n",
    "\\hat y = \\sigma\\left( W \\cdot x+b \\right)=\\sigma\\left(\\begin{pmatrix} w_0 & w_1\\end{pmatrix}\\cdot \\begin{pmatrix}x_0 \\\\ x_1\\end{pmatrix}+b\\right).\n",
    "$$\n",
    "Here the notation is the same as the one in the lecture presentation for this project. In this case, for each example, we take a linear combination $w_0x_0+w_1x_1+b$ of the input features $x_0$ and $x_1$), where $w_0,w_1,b$ are real numbers, followed by a *sigmoid* activation function $\\sigma(\\lambda)=\\displaystyle\\frac{1}{1+\\exp(-\\lambda)}$.  The output $\\hat y$ is the model's predicted probability that the point $x=\\begin{pmatrix}x_0 \\\\ x_1\\end{pmatrix}$ is blue. Here $W = \\begin{pmatrix} w_0 & w_1\\end{pmatrix}$ is known as the weight matrix and $b$ is the bias vector (which in this special case will actually be a single value, a $1\\times 1$ matrix).\n",
    "\n",
    "2. Initialize the model's trainable parameters $W$ and $b$.\n",
    "\n",
    "3. The training loop is then as follows:\n",
    "    - Implement forward propagation:  compute an array whose entries are $\\hat{y}^{(i)}$ for each training example, with the current values of $W$ and $b$. Here $\\hat{y}^{(i)}$ is the output probability for example in column $i$ of $X$.\n",
    "    - Compute the cost $J$ for the current values of the parameters.\n",
    "    - Implement backward propagation to get the gradients, i.e. the rate of change of the $J$ with respect to each of $w_0,w_1$ and $b$ at their current values.\n",
    "    - Update the parameters using these gradients. This is known as gradient descent.\n",
    "\n",
    "In practice, you'll build helper functions for each of these steps, and then merge them into one function called `model()`. Once you've built `model()` and used it to optimise the parameters, you can make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23p-xPYhrZar"
   },
   "source": [
    "#### Initialize parameters\n",
    "\n",
    "**Complete the function `initialize_parameters` started below.** It takes in the dimension of the input layer `n_X` and the dimension of the output layer `n_y` and returns returns a Python dictionary `parameters`. The dictionary `parameters` contains the initialized parameters of the model: the weight matrix `W` and the bias vector `b`.\n",
    "\n",
    "Note that `n_X = 2` in our example, each point has two input coordinates,  and the dimension of the output layer is `n_y = 1`. This means that `W` will be a $1\\times 2$ matrix and `b` will be a $1\\times 1$ matrix for our example. However we would like our function to work more generally, and be in a form which can be adapted for use later on in this project, so we'd like the user to be able to specify `n_X` and `n_y` as inputs to `initialize_parameters`.\n",
    "\n",
    "- Use: `np.random.randn(n,m) * 0.01` to randomly initialize a matrix of the required shape for `W`.\n",
    "- Initialize the bias vector `b` so that all of its entries are zero.\n",
    "- Return both of these in a dictionary called `parameters` with names *W* and *b* respectively.\n",
    "\n",
    "**Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CJLUZqQerZar"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_X, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_X -- integer\n",
    "    n_y -- integer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W -- weight matrix\n",
    "                    b -- bias vector\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialize weight matrix nd bias vector\n",
    "    W = None #you need to replace \"None\" with appropriate code\n",
    "    b = None #you need to replace \"None\" with appropriate code\n",
    "\n",
    "    #Store W and b in a dictionary parameters\n",
    "    parameters = {\"W\": W,\n",
    "                  \"b\": b}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ry0chdoVrZas"
   },
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "Next you will implement forward propagation for logistic regression:\n",
    "\n",
    "$$\n",
    "z = WX + b \\text{ and then } \n",
    "\\hat y = \\sigma(z)\n",
    "$$\n",
    "\n",
    "As above, for $\\lambda\\in\\mathbb{R}$, $\\sigma(\\lambda)=\\displaystyle\\frac{1}{1+\\exp(-\\lambda)}$ (this is applied entry-wise to $z$ in the above). \n",
    "\n",
    "**Start by implementing the sigmoid function in numpy by replacing *None* in the code cell below with appropriate code. Then run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TlrriuilOnKy"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    z -- input data, numpy array\n",
    "\n",
    "    Returns:\n",
    "    numpy array that contains sigmoid function applied to each\n",
    "    element of z.\n",
    "    \"\"\"\n",
    "    return None #you need to replace \"None\" with appropriate code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIpGGh2pbdQS"
   },
   "source": [
    "**Complete the function `forward_propagation` started below.** It calculates the output of the logistic regression model, called `y_hat`, and returns (as a tuple) `y_hat` and `z` (in the notation given above).\n",
    "\n",
    "**Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IH8ZYSFMrZat"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data, numpy array of dimension (2, 400). Every column of X\n",
    "         is the feature vector for a single training example (input of the logistic regression)\n",
    "    parameters -- python dictionary containing the values of W and b to be used\n",
    "                  to compute the forward propagation.\n",
    "\n",
    "    Returns:\n",
    "    y_hat -- numpy array of dimension (1,400). Every column of y_hat is the\n",
    "             output probability of the logistic regression for that training example.\n",
    "    z -- numpy array of dimension (1,400), result of linear transformation before sigmoid activation is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve W and b from the dictionary \"parameters\"\n",
    "\n",
    "    # compute z\n",
    "\n",
    "    # Compute y_hat\n",
    "\n",
    "    return y_hat, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTI9BdfPowOx"
   },
   "source": [
    "#### Compute cost\n",
    "\n",
    "Given the predictions $\\hat y^{(i)}$ on all the training examples, you can compute the cost $J$ as the average over all training examples of the losses:\n",
    "$$\n",
    "J = \\frac{1}{m} \\sum\\limits_{i = 0}^{m} L(\\hat y^{(i)}, y^{(i)})  .\n",
    "$$\n",
    "Here we are using the binary cross-entropy loss:\n",
    "$$\n",
    "L(\\hat y^{(i)}, y^{(i)}) = -\\large\\left(\\small y^{(i)}\\log\\left(\\hat y^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- \\hat y^{(i)}\\right)  \\large  \\right).\n",
    "$$\n",
    "\n",
    "Recall $y^{(i)}$ is the $i^{\\text{th}}$ component in $y$, i.e. the label for example $i$ and $\\hat{y}^{(i)}$ is the models current prediction of this label, the $i^{\\text{th}}$ component in $\\hat{y}$.\n",
    "\n",
    "**Complete the function `compute_cost()`, started below, to compute the value of the cost $J$:**\n",
    "\n",
    "1. First compute the vector of losses element-wise using the numpy arrays given as inputs `y_hat` and `y`.\n",
    "\n",
    "2. Sum the losses and divide by the number of training examples `m`. Call this value `cost`.\n",
    "\n",
    "3. Cast `cost` as type `float` using `float()` and return it.\n",
    "\n",
    "**Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r3efjlirrZat"
   },
   "outputs": [],
   "source": [
    "def compute_cost(y_hat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    y_hat -- numpy array, output of forward propagation\n",
    "    y -- numpy array of the same shape as y_hat, containing the true labels (0 for red, 1 for blue)\n",
    "\n",
    "    Returns:\n",
    "    cost -- a float which is the cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve number of training examples from the shape of y\n",
    "\n",
    "    # compute the vector of losses by computing the cross-entropy loss element-wise\n",
    "\n",
    "    # compute the total cost by averaging the loss over all training examples\n",
    "\n",
    "    # cast cost as a float\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFRzIlr6s5pb"
   },
   "source": [
    "#### Backpropagation\n",
    "\n",
    "You can now implement backward propagation. Note that you will complete the code cell below this, much of the code is already given to you but you need to stick with the notation established.\n",
    "\n",
    "NOTATION: in general for a generic numpy array `M` and a function $F$ with the entries of `M` as inputs, denote by `dM` the numpy array representing the gradient of $F$ with respect to `M`. More precisely `dM` contains the partial derivatives of $F$ with respect to the entries of `M`. For example if\n",
    "$$M = \\begin{pmatrix} a_{0,0} & a_{0,1} & a_{0,2} \\\\ a_{1,0} & a_{1,1} &  a_{1,2}\\end{pmatrix}$$ and $F$ is a function of $a_{0,0},a_{0,1},a_{0,2},a_{1,0},a_{1,1},a_{1,2}$ then $$dM = \\begin{pmatrix} \\frac{\\partial F}{\\partial a_{0,0}} & \\frac{\\partial F}{\\partial a_{0,1}} & \\frac{\\partial F}{\\partial a_{0,2}} \\\\ \\frac{\\partial F}{\\partial a_{1,0}} & \\frac{\\partial F}{\\partial a_{1,1}} & \\frac{\\partial F}{\\partial a_{1,2}}\\end{pmatrix}.$$\n",
    "\n",
    "Throught this project $F$ will be the cost function $J$.\n",
    "\n",
    "**Complete the function `backward_propagation()` started below.** It takes in a dictionary `parameters` containing the current parameters, `y_hat`,`X` and `y` and it returns a dictionary containing the gradients `dW` and `db` of the cost function with respect to the trainable parameters `W` and `b`. \n",
    "\n",
    "You are given the key code for calculating `dW` and `db`, it is, in the notation already established.\n",
    "\n",
    "`dz = y_hat - y\n",
    "dW = np.dot(dz, X.T)/m\n",
    "db = np.sum(dz, axis=1, keepdims=True)/m`\n",
    "\n",
    "You will see these lines of code in the cell below.\n",
    "\n",
    "**Once you have completed the code run this cell.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Kfoudw0NrZav"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, y_hat, X, y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing W and b to use\n",
    "    y_hat - from output of forward prop\n",
    "    X -- input data\n",
    "    y -- labels vector\n",
    "\n",
    "    Returns:\n",
    "    grads -- python dictionary containing the gradients\n",
    "    \"\"\"\n",
    "    # Retrieve the number of training examples from the shape of y\n",
    "    m = None #you need to replace None here.\n",
    "\n",
    "    # retrieve W from the dictionary \"parameters\".\n",
    "    W = None #you need to replace None here.\n",
    "\n",
    "    # Backward propagation: calculate dW, and db.\n",
    "    dz = y_hat - y\n",
    "    dW = np.dot(dz, X.T)/m\n",
    "    db = np.sum(dz, axis=1, keepdims=True)/m\n",
    "\n",
    "    grads = {\"dW\": dW,\n",
    "             \"db\": db}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMyWLnBXwGDj"
   },
   "source": [
    "#### Update parameters\n",
    "\n",
    "Next you will define a function to update parameters using gradient descent. In a similar way to that seen in task A2,\n",
    "\n",
    "1. $W$ will be updated to\n",
    "\n",
    "$$W - dW\\times \\text{learning rate}.$$\n",
    "\n",
    "and\n",
    "\n",
    "2. $b$ will be updated to$$b - db\\times\\text{learning rate}.$$\n",
    "\n",
    "**Complete the function `update_parameters()` started below.** It takes in the dictionary `parameters` containing the current parameters `W` and `b`, the dictionary `grads` containing the current gradient `dW`,`dB` and the learning rate (which defaults to the value $1.2$). **Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SDLtIC96rZaw"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters\n",
    "    grads -- python dictionary containing the gradients\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing the updated parameters\n",
    "    \"\"\"\n",
    "    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W\n",
    "    \n",
    "    W = copy.deepcopy(parameters[\"W\"])\n",
    "    b = None # you need to replace 'None' here\n",
    "\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "\n",
    "    # Update each parameter\n",
    "\n",
    "    # Store the new parameters in the dictionary parameters\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5nweAhNwpSn"
   },
   "source": [
    "#### Model\n",
    "\n",
    "Now you will put everything together into a function `model` where you will apply `update_parameters` iteratively to update the parameters a number of times.\n",
    "\n",
    "**Complete the function `model` started below.** It takes as inputs the training data `X`, `y`, a positive integer `num_iterations` which is the number of times the parameters will be updated (defaulting to $10,000$) and a boolean `print_cost` which, when set to `True`, will mean the function prints out some costs as the values of the parameters are updated). `model` return a dictionary containing the optimised parameters at the end of this process. **Once you have completed the code run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fktAz0CmrZaw"
   },
   "outputs": [],
   "source": [
    "def model(X, y, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- numpy array, input data\n",
    "    y -- numpy array, true labels\n",
    "    num_iterations -- number of iterations in the gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary which contains the optimized parameters\n",
    "                  learnt by the model.\n",
    "    \"\"\"\n",
    "    # Optional: to control the random seed of the initialization uncomment next line:\n",
    "    # np.random.seed(3)\n",
    "\n",
    "    # retrieve n_x and n_y from X and y\n",
    "\n",
    "    # Initialize parameters\n",
    "\n",
    "    # Training loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "\n",
    "        # Compute the cost\n",
    "\n",
    "        # Backpropagation\n",
    "\n",
    "        # Update parameters (use learning_rate = 1.2)\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "    learned_parameters = parameters\n",
    "    \n",
    "    # return learned parameters\n",
    "    return learned_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ9E2dBTFzgV"
   },
   "source": [
    "#### Predict on new data\n",
    "\n",
    "Below you are given a complete function `predict` that uses the learned parameters (output of `model`) and the `forward_propagation` function to predict a class for each example in the input matrix `X`. **Run the code cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sh78XGwPrZax"
   },
   "outputs": [],
   "source": [
    "def predict(learned_parameters, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the learned parameters given by model\n",
    "    X -- numpy array input data\n",
    "\n",
    "    Returns\n",
    "    predictions -- vector of predictions for each column of X (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute predicted probabilities using the learned parameters\n",
    "    \n",
    "    y_hat, z = forward_propagation(X, learned_parameters)\n",
    "\n",
    "    # Classify as 0 or 1 using 0.5 as a threshold\n",
    "    \n",
    "    predictions = (y_hat > 0.5)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu4daNs3Mfdd"
   },
   "source": [
    "#### Plot the results and calculate accuracy\n",
    "\n",
    "You are given the complete function `plot_decision_boundary` below which plots the decision boundary of the trained model along with the original data. It takes as arguments the `learned_parameters` (as output by the `model` function), the model, the training examples `X` and the true labels `y`. **Run the code cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fSqdDEgarZax"
   },
   "outputs": [],
   "source": [
    "# Function to plot decision boundary\n",
    "\n",
    "def plot_decision_boundary(learned_parameters, X, y):\n",
    "\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = predict(learned_parameters,np.c_[xx.ravel(), yy.ravel()].T)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x1')\n",
    "    plt.xlabel('x0')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EO8JcjrNUFj4"
   },
   "source": [
    "Now you can train your model using the training examples, and plot the decision boundary for the learned parameters, alongside the orginal data. **Run the code cell below**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ncPhjeRrZax"
   },
   "outputs": [],
   "source": [
    "# Train your model\n",
    "learned_parameters = model(X, y, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_decision_boundary(learned_parameters, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mr5va51Ydcs"
   },
   "source": [
    "As you should see logistic regression doesn't do a good job in predicting the labels for this dataset. This is because the dataset is not linearly separable, and therefore a simple logistic regression performs poorly on this data. \n",
    "\n",
    "**Complete the code cell below with a formula for the accuracy of the model**. This should be the precentage of points for which the model gives the correct classification. **Then run the cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc-R7PPGrZay"
   },
   "outputs": [],
   "source": [
    "# Print accuracy\n",
    "\n",
    "predictions = predict(learned_parameters, X)\n",
    "\n",
    "accuracy = None # replace 'None' with code which calculates the percentage of points for\n",
    "            # which the prediction agrees with the actual label.\n",
    "    \n",
    "print ('Accuracy of logistic regression: %d ' % accuracy +\n",
    "       '% ' + \"(percentage of correctly labelled datapoints)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwWhKThGwO9S"
   },
   "source": [
    "## A4 - Build a hidden layer Neural Network (worth approximately 20% of the marks)\n",
    "\n",
    "You are now going to build a full Neural Network with one hidden layer. This will allow to model to learn more complex decision boundaries and not just linear ones. This will result in an improved model compared to that you obtained in task A3. \n",
    "\n",
    "In practice you are going to repeat all the steps of task A3 but this time with an extra layer to the network. Set the hidden layer to have 4 neurons (in the code this dimension is denoted `n_h`). You may find it helpful to refer to the project lecture when you are completing this section.\n",
    "\n",
    "The forward propagation has the form `linear`->`tanh`->`linear`->`sigmoid`. So this will take the form below where $W$ and $T$ are appropriately sized matrices containing weights and $b$ and $c$ are appropriately sized column vectors for the biases (some of the dimensions of these objects will now depend on `n_h`). The input matrix $X$ is the same as in task A3, $z$, $a$, $v$ give intermediate output with $\\hat y$ the final output.\n",
    "\n",
    "$$\n",
    "z = W\\cdot X + b \\\\\n",
    "a = \\tanh(z) \\\\\n",
    "v = T \\cdot a + c \\\\\n",
    "\\hat y = \\sigma(v)\n",
    "$$\n",
    "\n",
    "The code for the new backward propogation function, in this notation, is as follows and you can cut and paste this into your code as appropriate.\n",
    "\n",
    "`dv = y_hat - y\n",
    "dT = np.dot(dv, a.T)/m\n",
    "dc = np.sum(dv, axis=1, keepdims=True)/m\n",
    "dz = np.dot(T.T, dv) * (1 - np.power(a, 2))\n",
    "dW = np.dot(dz, X.T)/m\n",
    "db = np.sum(dz, axis=1, keepdims=True)/m`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiUc-mOUwO9T"
   },
   "source": [
    "The cost is the same as for logistic regression, so there is no need to implement it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uE5OXXV9wO9X"
   },
   "source": [
    "#### Neural Network Model\n",
    "\n",
    "Below gives a basic structure for your final model, you need to build all the helper functions from scratch this time.\n",
    "\n",
    "**Specifically, using an approach similar to that seen in A3, you should**\n",
    "\n",
    "- Complete the function below `model_nn`, including all the necessary helper functions.\n",
    "- Include a function `predict` which takes in the dictionary of paremeters and input data 'X' and returns predictions of the labels for each example.\n",
    "- Produce a visualisation of the regions of the plane predicted to be red/blue by your model and the original data.\n",
    "- Calculate and print the accuracy of the model (the percentage of points for which the model predicts the correct label).\n",
    "- Make some comments, in a markdown cell, which compare the model developed in A3 to the model developed in A4.\n",
    "\n",
    "**You may use code from Task A3 here but be careful to rename functions and variables as appropriate to avoid clashes with any values established for this notebook in Task A3.**\n",
    "\n",
    "*Edit the code cell below and insert further code and markdown cells in which to answer this task*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjKhdq5SwO9X",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "461c83f5d15193186d8b122d0a451895",
     "grade": false,
     "grade_id": "cell-25f5e3e9b4ef006f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def model_nn(X, y, n_h = 4, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- numpy array, input data\n",
    "    y -- numpy array, true labels\n",
    "    n_h -- integer, number of neurons in the hidden layer\n",
    "    num_iterations -- number of iterations in the gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- optimized parameters learnt by the model.\n",
    "    \"\"\"\n",
    "    # Optional: to control the random seed of the initialization uncomment next line\n",
    "    # np.random.seed(3)\n",
    "\n",
    "    # retrieve n_x and n_y\n",
    "\n",
    "    # Initialize parameters\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "\n",
    "        # Compute cost\n",
    "\n",
    "        # Backpropagation\n",
    "\n",
    "        # Update parameters\n",
    "\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return learned_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdy5ZO9wCwAD"
   },
   "source": [
    "---\n",
    "### Section B (worth approximately 40%)\n",
    "\n",
    "Remember you are free to use sklearn in this section.\n",
    "#### Task B1\n",
    "\n",
    "- How can we further improve neural networks? You may wish to talk about underfitting, overfitting, regularisation and use examples to illustrate you ideas.\n",
    "\n",
    "#### Task B2\n",
    "- What are some other machine learning models (e.g. decision trees). Give some examples. What are improvements to these (e.g random forest, xgboost in the case of decision trees)?\n",
    "\n",
    "You may wish to use datasets from https://archive.ics.uci.edu/ in this section.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of overfitting, using 100 random points in $x \\in [0,1], y \\in [0,1]$ and the boundary function $$y \\ge \\left(\\frac{1}{3x}\\ +\\ \\frac{1}{6}x^{2}\\right)^{2}$$\n",
    "We categorise the points according to this boundary function but with some small random error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def overfitting_example() -> None:\n",
    "    N = 250\n",
    "    mid = int(N / 2)\n",
    "    test_size = 15\n",
    "    percent_wrong = 0.025\n",
    "\n",
    "    def boundary_func(x):\n",
    "        return (1 / (3*x) + (x**2) / 6)**2\n",
    "        \n",
    "    coords = np.array([np.random.random(N), np.random.random(N)])\n",
    "    categories = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        # For each point, check if it's above the boundary, but get some of them wrong\n",
    "        x, y = coords[:,i]\n",
    "        above_line = y >= boundary_func(x)\n",
    "\n",
    "        # Sometimes we want a point to be wrong, so we use the opposite value\n",
    "        if np.random.random() >= percent_wrong:\n",
    "            categories[i] = int(above_line)\n",
    "        else:\n",
    "            categories[i] = int(not above_line)\n",
    "\n",
    "    X_train = coords[:N - test_size]\n",
    "    X_test = coords[-test_size:]\n",
    "    \n",
    "    Y_train = categories[:N - test_size]\n",
    "    Y_test = categories[-test_size:]\n",
    "\n",
    "    red_coords = np.array([X_train[:,i] for i in range(N - test_size) if Y_train[i] == 1]).T\n",
    "    blue_coords = np.array([X_train[:,i] for i in range(N - test_size) if Y_train[i] == 0]).T\n",
    "\n",
    "    # Now plot things\n",
    "    plt.xlim(-0.02, 1.02)\n",
    "    plt.ylim(-0.02, 1.02)\n",
    "    \n",
    "    plt.plot(red_coords[0,:], red_coords[1,:], 'or')\n",
    "    plt.plot(blue_coords[0,:], blue_coords[1,:], 'xb')\n",
    "\n",
    "    xs = np.linspace(0.3, 1.02, 1001)\n",
    "    plt.plot(xs, boundary_func(xs))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "overfitting_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8URTe1gCwAD"
   },
   "source": [
    "## Notes about this submission\n",
    "\n",
    "You will submit a single Jupyter notebook for this project assignment. Details will be provided for this on the MA124 Moodle page.\n",
    "\n",
    "- The last thing you should do before submitting the notebook is to Restart Kernel and Run All Cells. You should then save the notebook and submit the .ipynb file. **You will lose marks if you submit a notebook that has not been run.**\n",
    "\n",
    "- You are expected to add code and markdown cells to this document as appropriate to provide your responses to the tasks. Instructions about this are given throughout but feel free to add markdown cells at any point to provide clarity or comments.\n",
    "\n",
    "- Likewise, to help the reader, please provide appropriate comments in your code (for example functions or blocks of code should have comments about what they do, variables should be described in comments, as appropriate)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
